{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Need\n",
    "The current steps include:\n",
    "Go to \"google patent\", key in the international publication number normally in the form of \"WO $$$$/$$$$$$ A1\".\n",
    "Click \"patentscope\" hyperlink.\n",
    "At the patentscope page, go to \"document\".\n",
    "\n",
    "In the document page, search for keywords \"amend+\" (meaning Amendments or amended claims, etc.)\n",
    "\n",
    "Here are two test cases: \n",
    "1) WO 2017/120441 A1 (space can be omitted) - with amendment; \n",
    "2) WO 2017/217183 A1 - without amendment.\n",
    "\n",
    "## Thoughts\n",
    "\n",
    "### Input (can be done later)\n",
    "1. read from csv for all patent_id need to be check\n",
    "2. Need to pre-pocess to take out only letters and number (e.g. WO2017/120441 to WO2017120441)\n",
    "\n",
    "### Process\n",
    "1. (don't know how to) There is API for WIPO at https://www.wipo.int/meetings/en/doc_details.jsp?doc_id=415578\n",
    "2. url = 'https://patentscope.wipo.int/search/en/detail.jsf?docId=' + 'The patent_id' + '&tab=PCTDOCUMENTS'\n",
    "3. request the url\n",
    "4. use bs4 to take out the txt under specific html\n",
    "5. grep those txt with \"amend\", save into a list\n",
    "6. return list\n",
    "\n",
    "### Output\n",
    "1. Print a list\n",
    "1.1 Column 1: patent_id\n",
    "1.2 Column 2: how many \"amend\" it have\n",
    "1.3 Column 3(if needed): info of each amend, like year-month, or type\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id = 'WO2017120441'\n",
    "url = 'https://patentscope.wipo.int/search/en/detail.jsf?docId='+patent_id+'&tab=PCTDOCUMENTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, features = 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_txt = soup.find_all('article',{'class': 'article-content'})\n",
    "len(All_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 \n",
    "# https://kite.com/python/examples/1716/beautifulsoup-extract-all-text-from-an-html-document\n",
    "import urllib.request\n",
    "uf = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(uf, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(soup.title.string)\n",
    "div = soup.find_all('div')\n",
    "sum(1 for _ in div)\n",
    "i = 0\n",
    "for div_i in div:\n",
    "    i += 1\n",
    "    if i == 20:\n",
    "        print (div_i.text)\n",
    "    elif i = 50:\n",
    "        break\n",
    "    \n",
    "\n",
    "# for div_i in div:\n",
    "#     print(div_i.text)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
